{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec4b57f2",
   "metadata": {},
   "source": [
    "## Basic relationship plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dae367c",
   "metadata": {},
   "source": [
    "Last time, we played around with plotting the distributions of variables, and comparing distributions to one another. Oftentimes, however two variables intimately related such that knowing a particular value of one variable allows you to predict, to some extent, the value of another variable. Say for example, two variables measured in the same sample or the same human individual. In cases where the values of two variables are interrelated, simply plotting the historgrams of each variable would not show the relationship. To show the relationship, we would want instead to plot the values of one variable against the values of the other. This plot (a.k.a. scatter plot) is best way to begin to appreciate the nature and strength of the relationship. So let's play around with plotting and evaluation some relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8265c2",
   "metadata": {},
   "source": [
    "##### As always, we'll start by importing the libraries we'll use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b588049",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b8949d",
   "metadata": {},
   "source": [
    "##### And read the first of our data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7a49db",
   "metadata": {},
   "outputs": [],
   "source": [
    "mdff = pd.read_csv(\"datasets/008TutorialDataFile1.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ca5c11",
   "metadata": {},
   "source": [
    "##### Now let's take a look to make sure we read something that looks okay\n",
    "\n",
    "Make sure you have read the instructions. As in the previous tutorials, we will assume that you will have imported the dataset under `./datasets`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5521f302",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(mdff)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c22948",
   "metadata": {},
   "source": [
    "#### We can now plot on varial (`X`) against the other (`Y`) to so visually appreciate their relationship.\n",
    "We can simply do this by using the built in plotting capability of the data frame created in `Cell [3]` when we loaded the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58404be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mdff.plot(x='X', y='Y', kind='scatter', color='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e754487e",
   "metadata": {},
   "source": [
    "OK, it looks like the two columns of the dataset we loaded, the variables `x` and `y`, are related. This can be interpreted as if the measurements in each row of the dataset have some sort of relationship.\n",
    "\n",
    "The plot shows the relationship. The relationship looks like a straight line in which `y` is related to `x` by `y = a + bx + err`, in which `a` is the y-intercept, `b` is the slope, and *`err`* represents 'error' or 'noise', that is, variablility in `y` that is unrelated to `x`. \n",
    "\n",
    "So, in a nutshell, it looks like if we know the value of `x`, we can predict the value of `y` except for a some amount of random variable represented by `err`. And all we need to know to do the predicting is two numbers, a slope and a y intercept.\n",
    "\n",
    "Now lets do a prettier plot using `seaborn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c929088",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme() # set theme to seaborn's default"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b83f057",
   "metadata": {},
   "source": [
    "Just like `displot()` is the seaborn quick and easy way for plotting distributions of variables, `relplot()` is the Q&E way for plotting relationships among variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff922d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.relplot(data=mdff, x='X', y='Y')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14082def",
   "metadata": {},
   "source": [
    "As we noted above, the data seem to fall around a roughly straight line. We can easily fit and plot that line using the `seaborn.lmplot()` function, where the `lm` in the call stands for \"linear model\" (of which a straight line is the simplest and default case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093528c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(data=mdff, x='X', y='Y')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f9ca95",
   "metadata": {},
   "source": [
    "So what happened here. In this plot, there is a straight line plotted along with the data. The `slope` and `y-intercept` of the line have been adjusted by the `lm` modelling method of `seaborn` so that the line does the best job at simultaneously coming the closest to all the data points. More specifically, the line shown is the line that minimizes the *sum of the squared differences between the line and the y data values*. \n",
    "\n",
    "This *sum of squared difference* between the line and the data points is also called *error*, because it represents the remaining difference between individual points and the linear model. It represents what is not accounted for by the model (the line) as as such is  considered error of the linear model. This error is generally referred to as the *sum squared error* or just the *sse*. \n",
    "\n",
    "The plot also shows the error bound around the fit. The error bound is represented as a semi-transparent shading around the line in the previous plot. The shading shows the uncertainty of the model. More specifcally, by assuming that the two samples (`x` and `y`) are representative of some underlying populations, or more generally of the underlying phenomena, if we were to obtain other samples from the same underlying populations that are producing the data, then 95% of experiments would produce data that would fit a straight line somewhere within these bounds. \n",
    "\n",
    "There are two main things we can take away from this plot. First, there is a relationship between the variables, and the data nail down that relationship very well (as indicated by the small error bounds).\n",
    "\n",
    "Second, there is noise – random variability – that limits our ability to predict *any one particular* y value from a give x value. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60a75ae",
   "metadata": {},
   "source": [
    "We can estimate the `slope` (or `b` in the equation above) and `y` intercept (or `a` in the equation above) of the straight line relationship with `numpy.polyfit()` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2d30d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "myfit = np.polyfit(x=mdff['X'], y=mdff['Y'], deg=1)\n",
    "print(myfit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a21320",
   "metadata": {},
   "source": [
    "The `deg=1` argument specifies that we want to fit the data with a *first order polynomial*, i.e., a straight line. The returned coefficients tell us that the `slope` is around 4.5 and the `y intercept` is around 240. But, of course, these numbers only allow to predict `y` *on average*. There is also the `error`, which puts a fundamental limit on our ability to predict any particular `y` value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71590486",
   "metadata": {},
   "source": [
    "We can explore the error a little more by looking at the *residuals*, which is just the difference (literally) between the line and the `y` values. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2e592e",
   "metadata": {},
   "source": [
    "Seaborn provides an easy way to peek at the rediduals using `seaborn.residplot()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e238225",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.residplot(data=mdff, x='X', y='Y')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fdda9e7",
   "metadata": {},
   "source": [
    "We can think of this as a picture of the noise alone; we have literally subtracted out the linear relationship with x! That means that for each value in `x` we have subtracted the value in `y` withg the corresponding value of the line. Let's think about it, in theory, if things go well with the model, meaning the line captures the data well then the subtraction should produce about half of the points above `0` and half below `0`. This is because if the model is good, unbiased then half of the `y` values should end up above the line and half below; this is because the line is supposed to pass right in the middle of the data! As we can see from the plot the distribution of the data is symmetric around `0`, and it is appears that half of the data points are above and half below `0`. \n",
    "\n",
    "After that we can look at the range or spread of the residuals. If we think that the model is a good one, the residuls should be small. This would indicate that the line is generally close to each data point. If the line were to be far from each datapoint then the line would not be very representative of the dataset and the residuals would spread across a wide range of values. A small or large spread (or range of values) of the residuals is indicative of the quality of the fit of the model. \n",
    "\n",
    "In our case, the plot shows that the range of the noise looks to be around 200 total in the `y` direction. If the variability is Gaussian, this would correspond to a standard deviation of around 200/6 or 33 (do you see why we divided by 6?). So we can say that we can predict y *on average* with a precision corresponding to a sigma of about +/- 33."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734eb54c",
   "metadata": {},
   "source": [
    "Let's explore this a little further by looking at the actual distribution of the residuals. In other words, lets look at the distribution of the noise component of `y`. We can do this by first getting the actual values of our best fit line at each value of `x` using `np.polyval()` – we just give it our fit from `np.polyfit()` and our x values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff51dae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fitvals = np.polyval(myfit, mdff['X'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b998f8b9",
   "metadata": {},
   "source": [
    "Now that we have those, we can compute the residual values by subtracting the best fit line from the y values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75b5a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "myres = mdff['Y'] - fitvals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44ee0c5",
   "metadata": {},
   "source": [
    "And then plotting those!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f6a59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(myres, kind='kde')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177c8800",
   "metadata": {},
   "source": [
    "Sure enough, looks like a Gaussian distribution with a standard deviation of around 30 or so. Or, more precisely:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86396040",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.std(myres)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83a48e9",
   "metadata": {},
   "source": [
    "Okay, so, what have we done? We have looked at the data and then created a simple model in which `y` is a linear function of `x` plus random variability. If we let *N*(mu, sigma) stand for a normal distribution with a mean of mu and a standard deviation of sigma, the we can actually write down our as\n",
    "\n",
    " *y = 4.5 * x + 242 + N(0, 36)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe67047",
   "metadata": {},
   "source": [
    "Even though this is a simple line fit, the basic procedure we just followed is a general procedura that scientists and data scientists always use (or should use) when presented with a new dataset to explore and model. The procedure is the same no matter how complicated a situation or dataset we are dealing with:\n",
    "\n",
    "1. look at the data\n",
    "2. make a guess at the relationship (perhaps using prior knowledge in addition to the data, start with something simple, say a linear relationship)\n",
    "3. fit a model to the data\n",
    "4. evaluate how well the model fits the data (look at the error and residuals)\n",
    "\n",
    "Sometimes these steps are informal and internal (Oh, yeah, that's linear), and sometimes we go way down into the weeds in the fitting and evaluation, but these are the basic steps!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5358f892",
   "metadata": {},
   "source": [
    "##### Let's look at a second set of data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b52550",
   "metadata": {},
   "outputs": [],
   "source": [
    "mdff = pd.read_csv(\"datasets/008TutorialDataFile2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a434f4f1",
   "metadata": {},
   "source": [
    "Take a peek:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4f5e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(mdff)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c909cfa",
   "metadata": {},
   "source": [
    "Plot the relationship!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620a07ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.relplot(data=mdff, x='X', y='Y')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f033ee",
   "metadata": {},
   "source": [
    "So these data look both the same and different than the last data set. They look the same in that it looks like there is a linear relationship between x and y. Let's look at that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76db723",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(data=mdff, x='X', y='Y')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0074020",
   "metadata": {},
   "source": [
    "So it looks like we have a nice linear relationship as before, but the slope is perhaps not pinned down as well (note the bow tie shape of the error bounds). Cool."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57758855",
   "metadata": {},
   "source": [
    "But these data also look different if we look at the x values. In the last data set, the x values were evenly spaced, as though they came from a laboratory experiment in which x was intentionally adjusted in a precise way. In this data set, it looks as though x, like y, was randomly sampled.\n",
    "\n",
    "So let's plot these data in some ways that we can look at both x and y by making a scatterplot and then adding a \"rug\" plot along the axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f090d96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=mdff, x='X', y='Y')\n",
    "sns.rugplot(data=mdff, x='X', y='Y')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e27a95c",
   "metadata": {},
   "source": [
    "The rugs are basically interior tic marks showing the positions of each data point close to the corresponding axis. Here, we can see that both variables cluster near the centers and get more sparse towards the edges of the plot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187d6f2b",
   "metadata": {},
   "source": [
    "Because both x and y seem to be random variables, the scatter plot above shows the *joint distribution* of x and y. We can take a more detailed look by plotting both the joint distribution, and the individual or *marginal* distributions of the two variables in the *margins* of the plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5444ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(x=\"X\", y=\"Y\", data=mdff)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d84814d",
   "metadata": {},
   "source": [
    "Oh! That's pretty! And we can see at a glance that each variable is distributed roughly as Gaussians as well as seeing the y vs. x relationship. We can also make a version with the best fit line by specifying the `kind` argument to `reg` for regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa25a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(x=\"X\", y=\"Y\", data=mdff, kind=\"reg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e733743b",
   "metadata": {},
   "source": [
    "As a bonus, this also seems to add KDE plots to the marginal distributions!\n",
    "\n",
    "So, looking at these data, we'd conclude that there is a positive linear relationship between x and y, though both variables are noisy. If we wished, we could of course do a deeper dive into a linear model and how well it fits!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87241fbc",
   "metadata": {},
   "source": [
    "##### Let's look at our third and final dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4935e436",
   "metadata": {},
   "outputs": [],
   "source": [
    "mdff = pd.read_csv(\"datasets/008TutorialDataFile3.csv\")\n",
    "display(mdff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeaf7afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.relplot(data=mdff, x='X', y='Y')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17cde91a",
   "metadata": {},
   "source": [
    "So, again, we have a strong relationship. Let's do our plot with a simple model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288bf8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(data=mdff, x='X', y='Y')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22fb4936",
   "metadata": {},
   "source": [
    "The simple line fit looks okay, and the error bounds on the line are small. Great! Right!\n",
    "\n",
    "If we look a little more closely at the actual data though, without getting seduced by the line, we see that the relationship here is perhaps a little more complicated. In addition to y going up with x, it looks like there's a curve in the data such that, the higher x is, the faster y increases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00726a4",
   "metadata": {},
   "source": [
    "Let's look at the residuals from our line fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0023d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.residplot(data=mdff, x='X', y='Y')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0f1c01",
   "metadata": {},
   "source": [
    "Here we can clearly see that there is a *pattern to the residuals*. The residuals do not look like how expected, meaning they are not centered around zero or randoml distributed around zero. There is a shape, a banana shape we would say. This is diagnostic. In general, true error, true random variability tends to be normally distributed (thank you Central Limit Theorem). Thus, if our model is really capturing the data, then the residuals should be flat and normally distributed around zero (think about it). So this \"smile\" pattern in the data is because our model is systematically overestimating the data in the middle and underestimating at the two extremes.\n",
    "\n",
    "It looks like the data are bending but our model isn't.\n",
    "\n",
    "We'll talk about different kinds of models as we go on, but a very simple way to capture a bend in data is to expand our *first degree polynomial* model – a straight line – to a *second order polynomial* in which\n",
    "\n",
    " *y = a + bx + cx^2 + N(0,1)*\n",
    "\n",
    " The squared term turns our carrot model into a banana model. Let's look!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbdcc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(data=mdff, x='X', y='Y', order=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60922238",
   "metadata": {},
   "source": [
    "Note the `order=2` argument in `lmplot()`\n",
    "\n",
    "You might be thinking \"Wait, didn't `lm` stand for *linear model*?\" But now we're squaring x, doesn't that make it non-linear\"? Indeed, all polynomials are actually linear models by definition – not all lines are straight!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5414f7",
   "metadata": {},
   "source": [
    "Let's look at the residuals again!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ebba4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.residplot(data=mdff, x='X', y='Y', order=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6ca8af",
   "metadata": {},
   "source": [
    "Okay, that looks good now! But let's go ahead and take a slightly deeper dive like we did above, but without the color commentary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49910c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "myfit = np.polyfit(x=mdff['X'], y=mdff['Y'], deg=2)\n",
    "print(myfit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9f4c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "fitvals = np.polyval(myfit, mdff['X'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072bd136",
   "metadata": {},
   "outputs": [],
   "source": [
    "myres = mdff['Y'] - fitvals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f02bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(myres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9adc47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.std(myres)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8fd0166",
   "metadata": {},
   "source": [
    "So now we can say that the residuals now look like truly random noise, and that the data are captured by\n",
    "\n",
    " *y = 7.3 + 0.02x + 0.7x^2 + N(0,98)*\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71d690d",
   "metadata": {},
   "source": [
    "So, here, we played all the same games that we talked about above. The difference is that we tried a candidate model, a straight line, decided it wasn't quite right, and then settled on a curvy model instead.\n",
    "\n",
    "Now, as you might have realized already, the models we played with in this tutorial, were purely *descriptive*. We don't know what `x` and `y` are, and can't therefore say anything about *why* `y` should be related to `x`. Ultimately, we want models that represent the things that are generating the data, not just ones that reasonably describe the data.\n",
    "\n",
    "Ultimately, though, the basic process of fitting and evaluation are the same, so this tutorial gives us a good start!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bda1f12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cd78fef2128015050713e82ca51c6520b11aee7c9ee8df750520bbbc7384cbaa"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
